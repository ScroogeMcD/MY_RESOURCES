# KAFKA


## Kafka - installation locations on my Macbook
* the log dir (directory where the topic-partition log files are stored) is defined at ```/usr/local/etc/kafka/server.properties``` by the name ```log.dirs```
* The log dir mentioned by default in the above location is ``/usr/local/var/lib/kafka-logs```
* Under this logs directory, there are files with the name format as <topicName>-<partitionNumber>, for ex. stockPriceTopic-0
* Under this logs directory, there is a special topic called ```__consumer_offsets``` which mantains the last committed offset per consumerGroup and partition.

## Kafka CLI
* To list down the kafka topics </br>
``kafka-topics --bootstrap-server localhost:9092 --list``
* To create a topic</br> 
``kafka-topics --bootstrap-server localhost:9092 --topic stockPriceTopic -- create --partitions 3 --replication-factor 1``</br>
  We cannot have a replication factor of more than 1, if we have only one available broker in our cluster
* To describe a topic </br>
``kafka-topics --bootstrap-server localhost:9092 --topic stockPriceTopic --describe``

## Producer 
To connect to the Kafka broker, we need to set come configurations in a ```java.util.Properties``` object :
```java
Properties props = new Properties();
props.put("bootstrap.servers",bootstrapServers);  // bootstrapServers is a string consisting of comma-separated localhost:port addresses of brokers
props.put("acks","all");
props.put("key.serializer","com.example.base.domain.StockPriceKeySerializer");
props.put("value.serializer","com.example.base.domain.StockPriceSerializer");
props.put("partitioner.class","com.example.base.stockproducer.StockBasedPartitioner");
kafkaProducer = new KafkaProducer<>(props);
```

API to write to a topic on this broker cluster:
```java
// (topicName,key,value) is required as params
ProducerRecord<StockPriceKey, StockPrice> record = new ProducerRecord<>(TOPIC_NAME_STOCK_PRICE, sp.getStockPriceKey(), sp); 
kafkaProducer.send(record, new Callback() {
  @Override
  public void onCompletion(RecordMetadata metadata, Exception exception) {
    System.out.print("Key : \t" + sp.getStockPriceKey());
    System.out.print("Topic : \t" + metadata.topic() + "\t");
    System.out.print("Partition : \t" + metadata.partition() + "\t");
    System.out.println("Offset : \t" + metadata.offset());
  }
});
```
In the ProducerRecord constructor,
* If a valid partition number is provided, then the record is appended to that partition.
* If no partition number is provided but a key is used, then the partition is chosen based on the hash of the key.
* If neither key nor partition is present, then the partition is assigned in a round-robin fashion.

Different constructors to create a producer record :
```java
// API 1
public ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value, Iterable<Header> headers)

// API 2
public ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)

// API 3
public ProducerRecord(String topic, Integer partition, K key, V value, Iterable<Header> headers)

// API 4
public ProducerRecord(String topic, Integer partition, K key, V value)

// API 5
public ProducerRecord(String topic, K key, V value)

// API 6
public ProducerRecord(String topic, V value)
```


## Consumer
* **STEP 01 : Properties to create a kafkaConsumer object**
```java
private KafkaConsumer<StockPriceKey, StockPrice> kafkaConsumer;

Properties props = new Properties();
props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, Constants.bootstrapServers);
props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,"com.example.base.domain.StockPriceKeyDeserializer");
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, "com.example.base.domain.StockPriceDeserializer");
props.put(ConsumerConfig.GROUP_ID_CONFIG, consumerGroupId);
props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "4");
props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

kafkaConsumer = new KafkaConsumer<>(props);
```
* **STEP 02 : subscribe to the topic**
```java
kafkaConsumer.subscribe(Arrays.asList(Constants.TOPIC_NAME_STOCK_PRICE));
```
* **STEP 03 : poll the broker**
```java
while(true){
  ConsumerRecords<StockPriceKey, StockPrice> records = kafkaConsumer.poll(Duration.ofMinutes(1));
  for(ConsumerRecord<StockPriceKey, StockPrice> record : records){
    System.out.println(record.key() + "\t" + record.value());
    System.out.println("Partition : " + record.partition() + ",\t offset: " + record.offset());
  }
}
```

There is no background thread in the java consumer. The API depends on calls to ```poll()``` to drive all of its I/O, including :
* joining the consumer group and handling parition rebalances
* sending periodic heartbeats if part of an active generation
* sending periodic offset commits (if autocommit is enabled)
* sending and receving fetch requests for assigned partitions

## Consumer Groups
A consumer group is a set of consumers which cooperate to consume data from some topics. The partitions of all the topics are divided among the consumers in the group. As new group members arrive, and old group members leave, the partitions are reassigned so that each member receives a proportional share of the partitions. This is known as rebalancing the group.

## Offset Management
The internal topic  ```_consumer_offsets``` is used to store committed offsets.
After the consumer receives its assignment from the coordinator, it must determine the initial position for each assigned partition. 
As a consumer in the group reads messages from the partitions assigned by the coordinator, it must commit offsets corresponding to the messages it has read. If the consumer crashes or is shut down, its partitions will be re-assigned to another member, which will begin consumption from the last committed offset of each partition.

The offset commit policy is crucial to providing the message delivery guarantees. By default, the consumer is configured to use an automatic commit policy, which triggers a commit on a periodic interval. Using 'auto-commit' gives us ```at-least-once``` delivery.

## Partition Rebalancing
* Within a ConsumerGroup, each partition of a topic is exclusively assigned to one consumer.
* If more consumers join the ConsumerGroup, and if there are more number of partitions than the number of consumers, then the partitions are rebalanced
so that some of the existing partitions are moved to new consumers, to distribute the load.
* If there are more number of consumers than the number of partitions, then some of the consumers would remain idle. 

## Kafka delivery guarantees

## Transactions

## Useful links
* https://docs.confluent.io/clients-kafka-java/current/overview.html
* https://www.confluent.io/blog/transactions-apache-kafka/
