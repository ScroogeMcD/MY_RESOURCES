# Storage structures in distributed systems
* [Inverted Index](#inverted-index)
* [Merkle Trees](#merkle-trees)
* [HyperLogLog](#hyperloglog)
* [Count Min Sketch](#count-min-sketch)
* [Skip Lists](#skip-lists)
* [B+ Trees](#b+-trees)

## Inverted Index
Assume we have the following ``documents`` table in a database :

|DocId|Document|
|-----|--------|
|101| This winter is very harsh.|
|102| Harsh measures are required|
|103| winter is coming|
|104| very good|

Now if somebody fires a query to find all the DocIds where the word ``winter`` is present, a crude way would be to scan through all the document rows, and find out which one contains this string.
```sql
select DocId
from documents
where Document like '%winter%'
```

Alternatively, we could have built an inverted index of all the important words (excluding words like a, an, of, the, etc.), which would be a map of word to the coreesponding DocIds where it is present. This would help us quickly find the relevant DocIds.   
The inverted index, in our case, would look like : 
|word| DocIds|
|----|-------|
|this|101|
|winter|101, 103|
|harsh|101, 102|
|very|104|
|good|104|
|measures|102|

## Merkle Trees

## HyperLogLog

## Count Min Sketch
[Reference](https://florian.github.io/count-min-sketch/)   
This is a probabilistic data structure similar to Bloom Filters. We want to count the frequency of distinct elements in a stream.
One way to do this is maintaining a HashMap and increment the count for each occurrence of a key. However this will require a large amount of space, proportional
to the number of distinct keys in the stream.   
If we do not want the exact count, and can live with a close approximation, then a probabilistic strcuture like **Count-Min Sketch** can be used.

Let us start with an array of fixed size (say *m*), and have a HashFunction *h*. Everytime we get an element, we pass it via the hash function, find the corresponding location in the array, and then increment the count at that location by one. However the more the number of different elements getting hashed to the same location (**collision**), the more inaccurate our count would become.   
To solve this, we can try two things :
* **Option 1 : Increase the number of hash functions, for this single array of size *m*** : This will cause even more collisions across keys, so this will cause our count to be all the more inaccurate, than when we had just one hash function.
* **Option 2 : Increse the number of hash functions, and have one array for each hash function** : Instead of the earlier approach of having one array across all hash functions, we will now have one array for each hash function. So increasing hash functions will no longer collisions. And to reduce the inaccuracy due to collisions in one hash function, while querying the count of an element, we will hash it through all the functions, find the corresponding counts, and take the minimum of it. The matrix formed in this structure is called a sketch.

![CountMinSketch-2](https://user-images.githubusercontent.com/13499858/148096179-55195579-e3d5-4663-81c4-9ee68289c4da.png)


## Skip Lists

## B+ Trees
