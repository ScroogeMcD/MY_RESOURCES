# Spark RDD

```Java
public class Main {

    public static void main(String[] args) throws IOException {
        Logger.getLogger("org.apache").setLevel(Level.WARN);

        Set<String> boringWords = new HashSet<>();
        InputStream is = Main.class.getResourceAsStream("/subtitles/boringwords.txt");
        BufferedReader br = new BufferedReader(new InputStreamReader(is));
        String line;
        while((line = br.readLine()) != null){
            boringWords.add(line);
        }

        SparkConf conf = new SparkConf().setAppName("MyFirstSparkApp"); //.setMaster("local[*]");
        JavaSparkContext sc = new JavaSparkContext(conf);

        JavaRDD<String> initialRDD = sc.textFile("s3://udemy-spark-demos-sarvesh/input.txt");
        //JavaRDD<String> initialRDD = sc.textFile("/Users/priyanka/Desktop/MyProjects/SparkProject/src/main/resources/subtitles/input.txt");

        initialRDD.flatMap(sentence -> Arrays.stream(sentence.split(" ")).iterator())
                .map(sentence -> sentence.replaceAll("[^a-zA-Z]",""))
                .filter(word -> word.length() > 1)
                .map(w -> w.toLowerCase(Locale.ROOT))
                .filter(word -> !boringWords.contains(word))
                .mapToPair(word -> new Tuple2<String, Integer>(word, 1))
                .reduceByKey((v1, v2) -> v1 + v2)
                .mapToPair(pair -> new Tuple2<Integer, String>(pair._2, pair._1))
                .sortByKey(false)
                .take(10)
                .stream().forEach(p -> System.out.println("Word : " + p._2 + "\t Count: " + p._1));

        sc.close();

    }
}
```

# Spark SQL

# Spark ML
